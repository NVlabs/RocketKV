{
    "pipeline_params": {
        "model_name": "togethercomputer/Llama-2-7B-32K-Instruct",
        "tokenizer_name": "togethercomputer/Llama-2-7B-32K-Instruct",
        "chat_template": "llama2",
        "model_max_len": 31500,
        "fattn": true,
        "use_flash_attn": true,
        "truncation_mode": "middle",
        "batch_size": 1,
        "distance_scale": 1.0,
        "out_of_max_len_allowed": true,
        "base": 10000,
        "skip_layers": 2,
        "rope_theta_factor": 1.0,
        "rope_linear_scaling_factor": 8.0
    }
}
